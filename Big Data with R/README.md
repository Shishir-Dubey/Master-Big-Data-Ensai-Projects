# Big Data with R
# Ensai Course Project

Objective : 

Performing large-scale computation is very difficult. To work with this volume of data
requires distributing parts of the problem to multiple machines to handle in parallel.
Whenever multiple machines are used in cooperation with one another, the probability of
failures rises. In a single-machine environment, failure is not something that program
designers explicitly worry about very often: if the machine has crashed, then there is no way
for the program to recover anyway. In a distributed environment, however, partial failures
are an expected and common occurrence. Synchronization between multiple machines
remains the biggest challenge in distributed system design. When it comes to Big Data this
proportion is turned upside down. Big Data comes into play when the CPU time for the
calculation takes longer than the cognitive process of designing a model. The number of
records of a data set is just a rough estimator of the data size though. Itâ€™s not about the size
of the original data set, but about the size of the biggest object created during the analysis
process. Depending on the analysis type, a relatively small data set can lead to very large
objects.


